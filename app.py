# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10n11K_ANoABNvG8WNzXxuYNWuclFiTCp
"""

# app.py
"""
Educational Streamlit app demonstrating:
 - Data preparation for MovieLens-style ratings (UserID::MovieID::Rating::Timestamp)
 - Manual construction of a dense user-item matrix (after remapping IDs)
 - Manual computation of item averages and user effects (with optional Laplace noise)
 - Manual SGD matrix factorization (explicit loops) to learn P and Q
 - Predicting ratings and recommending movies similar in predicted rating
This is written step-by-step for teaching; it's intentionally simple and verbose.
Disclaimer: Laplace noise here is illustrative, not a production DP implementation.
"""

import streamlit as st
import pandas as pd
import numpy as np
from io import StringIO
from typing import Tuple

st.set_page_config(page_title="Educational MF + Laplace Noise (Streamlit)", layout="wide")
st.title("Educational Matrix Factorization + Laplace Noise (Manual SGD)")

###############################################################################
# ----------------------------- Helper functions -----------------------------
###############################################################################

def read_ratings(file_like) -> pd.DataFrame:
    """Read MovieLens-style ratings.dat with '::' delimiter robustly."""
    cols = ['UserID', 'MovieID', 'Rating', 'Timestamp']
    df = pd.read_csv(file_like, sep='::', names=cols, engine='python')
    return df

def remap_ids(df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:
    """
    Map raw UserID and MovieID to contiguous indices 0..n-1.
    Add columns 'u_idx' and 'i_idx' to df and return unique user/movie arrays.
    """
    df = df.copy()
    df['u_idx'], users = pd.factorize(df['UserID'])
    df['i_idx'], items = pd.factorize(df['MovieID'])
    return df, users, items

def build_dense_matrix(df: pd.DataFrame, n_users: int, n_items: int) -> np.ndarray:
    """
    Build a dense user-item matrix R with zeros where no rating exists.
    (Good for small datasets; for large datasets use sparse storage).
    """
    R = np.zeros((n_users, n_items), dtype=float)
    for _, row in df.iterrows():
        u = int(row['u_idx']); i = int(row['i_idx'])
        R[u, i] = row['Rating']
    return R

def laplace_noise(scale: float, shape=1):
    """Return Laplace noise (numpy) with given scale and shape."""
    return np.random.laplace(loc=0.0, scale=scale, size=shape)

def compute_item_averages(R: np.ndarray, beta_i: float = 0.0,
                          add_laplace: bool = False, laplace_scale: float = 0.1,
                          r_min: float = 1.0, r_max: float = 5.0) -> np.ndarray:
    """
    Compute item averages using only observed ratings (non-zero entries).
    Optionally add Laplace noise to each item average (educational).
    Regularization via beta_i.
    """
    n_items = R.shape[1]
    item_avgs = np.zeros(n_items)
    for j in range(n_items):
        col = R[:, j]
        observed = col[col > 0]
        if observed.size == 0:
            # If no ratings, set to mid-point of rating range (or global mean could be used)
            avg = (r_min + r_max) / 2.0
        else:
            avg = observed.sum() / observed.size
            # regularize towards global mean if beta provided
            # (simple shrinkage toward global mean)
        item_avgs[j] = avg
    # regularization/shrinkage across all items
    if beta_i > 0:
        global_mean = np.sum(R[R > 0]) / np.count_nonzero(R)
        counts = np.array([(R[:, j] > 0).sum() for j in range(n_items)])
        # shrinkage: (sum + beta*global_mean) / (count + beta)
        for j in range(n_items):
            c = counts[j]
            if c == 0:
                item_avgs[j] = global_mean
            else:
                item_avgs[j] = (item_avgs[j] * c + beta_i * global_mean) / (c + beta_i)

    if add_laplace:
        item_avgs = item_avgs + laplace_noise(scale=laplace_scale, shape=item_avgs.shape)
    # clamp
    item_avgs = np.clip(item_avgs, r_min, r_max)
    return item_avgs

def compute_user_effects(R: np.ndarray, item_avgs: np.ndarray, beta_u: float = 0.0,
                         add_laplace: bool = False, laplace_scale: float = 0.1,
                         clamp_min: float = -2.0, clamp_max: float = 2.0) -> np.ndarray:
    """
    Compute user effects (residuals after removing item averages).
    user_avg_v = average of (r_ui - item_avg_i) across i rated by user v.
    Optionally add Laplace noise (illustrative).
    """
    n_users = R.shape[0]
    user_effects = np.zeros(n_users)
    for u in range(n_users):
        row = R[u, :]
        rated_idx = row > 0
        if rated_idx.sum() == 0:
            user_effects[u] = 0.0
        else:
            residuals = row[rated_idx] - item_avgs[rated_idx]
            user_effects[u] = residuals.mean()
    # simple shrinkage towards 0 with beta_u
    if beta_u > 0:
        user_effects = user_effects / (1.0 + beta_u)

    if add_laplace:
        user_effects = user_effects + laplace_noise(scale=laplace_scale, shape=user_effects.shape)

    user_effects = np.clip(user_effects, clamp_min, clamp_max)
    return user_effects

def rmse_on_observed(R_true: np.ndarray, R_pred: np.ndarray) -> float:
    """Compute RMSE using only observed (non-zero) entries in R_true."""
    mask = R_true > 0
    if mask.sum() == 0:
        return float('nan')
    diff = R_true[mask] - R_pred[mask]
    return np.sqrt(np.mean(diff ** 2))

def manual_sgd_factorization(R: np.ndarray, d: int = 5, epochs: int = 20,
                             lr: float = 0.01, reg: float = 0.02,
                             add_laplace_to_error: bool = False, laplace_scale: float = 0.1,
                             verbose: bool = True) -> Tuple[np.ndarray, np.ndarray, list]:
    """
    Manual SGD MF:
     - Initialize P (users x d) and Q (items x d)
     - For each observed rating r_ui, predict p_u dot q_i, compute error e_ui = r_ui - pred
     - Optionally add Laplace noise to error (illustrative)
     - Update:
         p_u += lr * (e_ui * q_i - reg * p_u)
         q_i += lr * (e_ui * p_u_old - reg * q_i)
    Returns P, Q, and list of rmse per epoch.
    """
    n_users, n_items = R.shape
    rng = np.random.RandomState(42)
    P = 0.1 * rng.normal(size=(n_users, d))
    Q = 0.1 * rng.normal(size=(n_items, d))

    # Precompute list of observed entries for faster looping
    obs = [(u, i, R[u, i]) for u in range(n_users) for i in range(n_items) if R[u, i] > 0]
    rmse_history = []

    for epoch in range(epochs):
        rng.shuffle(obs)
        for (u, i, r_ui) in obs:
            # prediction
            pred = np.dot(P[u], Q[i])
            # error
            e_ui = r_ui - pred
            if add_laplace_to_error:
                e_ui = e_ui + np.random.laplace(scale=laplace_scale)
            # store old P[u] for symmetric update
            p_u_old = P[u].copy()
            # SGD updates
            P[u] += lr * (e_ui * Q[i] - reg * P[u])
            Q[i] += lr * (e_ui * p_u_old - reg * Q[i])

        # After epoch compute full predicted matrix and RMSE on observed
        R_pred = P.dot(Q.T)
        current_rmse = rmse_on_observed(R, R_pred)
        rmse_history.append(current_rmse)
        if verbose:
            st.write(f"Epoch {epoch+1}/{epochs} — RMSE on observed = {current_rmse:.4f}")
    return P, Q, rmse_history

def predict_rating_user_item(P: np.ndarray, Q: np.ndarray, item_avgs: np.ndarray,
                             user_effects: np.ndarray, u_idx: int, i_idx: int,
                             r_min: float = 1.0, r_max: float = 5.0) -> float:
    """Predict rating for single user u_idx and item i_idx."""
    pred_latent = np.dot(P[u_idx], Q[i_idx])
    pred = pred_latent + item_avgs[i_idx] + user_effects[u_idx]
    return float(np.clip(pred, r_min, r_max))

def recommend_similar_by_predicted_rating(P: np.ndarray, Q: np.ndarray, item_avgs: np.ndarray,
                                          user_effects: np.ndarray, R: np.ndarray, u_idx: int,
                                          given_i_idx: int, top_n: int = 10) -> pd.DataFrame:
    """
    For user u_idx, compute predicted ratings for all items and return top_n items
    whose predicted rating is closest to predicted rating of given_i_idx.
    Exclude items already rated by user.
    Return DataFrame with columns: i_idx, predicted_rating, abs_diff.
    """
    n_items = R.shape[1]
    preds = (P[u_idx] @ Q.T) + item_avgs + user_effects[u_idx]
    # clamp
    preds = np.clip(preds, 1.0, 5.0)
    given_pred = preds[given_i_idx]
    abs_diff = np.abs(preds - given_pred)
    # exclude the given movie itself and already-rated items
    abs_diff[given_i_idx] = np.inf
    abs_diff[R[u_idx] > 0] = np.inf
    top_idx = np.argsort(abs_diff)[:top_n]
    return pd.DataFrame({
        'i_idx': top_idx,
        'PredictedRating': preds[top_idx],
        'AbsDiffFromGiven': abs_diff[top_idx]
    })

###############################################################################
# ------------------------------ Streamlit UI --------------------------------
###############################################################################

st.sidebar.header("Data options")
use_sample = st.sidebar.checkbox("Use sample MovieLens (small) ratings.dat file", True)

# if use_sample:
#     # Small embedded sample (first 3000 rows would be fine; but keep tiny for speed)
#     # For real use, place ratings.dat in data/ and uncheck sample
#     sample_text = """1::1193::5::978300760
# 1::661::3::978302109
# 1::914::3::978301968
# 1::3408::4::978300275
# 1::2355::5::978824291
# 2::1193::3::978298413
# 2::661::4::978298377
# 2::914::2::978298478
# 3::1193::4::978220179
# 3::661::5::978220179
# """
if use_sample:
    uploaded_file = StringIO(sample_text)
    st.sidebar.write("Using a tiny embedded sample. For better learning, upload the MovieLens 100k ratings.dat.")
else:
    uploaded_file = st.sidebar.file_uploader(
        "Upload ratings.dat (MovieLens style)", 
        type=['dat', 'csv', 'txt']
    )
    if uploaded_file is None:
        st.warning("Upload a ratings.dat file or select the sample option from the sidebar.")
        st.stop()

# Read and show raw data
df_raw = read_ratings(uploaded_file)
st.subheader("Raw ratings (first few rows)")
st.dataframe(df_raw.head())


# Data cleaning / prep
st.markdown("### Step 1 — Data preparation & remapping IDs")
df, users, items = remap_ids(df_raw)
n_users, n_items = len(users), len(items)
st.write(f"Number of unique users: {n_users}, unique items: {n_items}")
st.write("After remapping, first rows with indices:")
st.dataframe(df[['UserID', 'u_idx', 'MovieID', 'i_idx', 'Rating']].head())

# Build the dense user-item matrix (teaching use only)
st.markdown("### Step 2 — Build dense user-item matrix (zeros = unrated)")
R = build_dense_matrix(df, n_users, n_items)
st.write("Matrix shape:", R.shape, " — (users x items)")
st.write("Nonzero ratings (observed):", np.count_nonzero(R))

# Item averages and user effects
st.markdown("### Step 3 — Compute item averages and user effects (with optional Laplace noise)")
st.sidebar.header("Bias & noise options (educational)")
beta_i = st.sidebar.number_input("Item shrinkage beta_i (0 = no shrinkage)", min_value=0.0, max_value=100.0, value=10.0)
beta_u = st.sidebar.number_input("User shrinkage beta_u (0 = no shrinkage)", min_value=0.0, max_value=100.0, value=0.0)
add_noise_to_biases = st.sidebar.checkbox("Add Laplace noise to item/user biases (illustrative)", value=True)
bias_noise_scale = st.sidebar.number_input("Laplace scale for bias noise", min_value=0.0, max_value=2.0, value=0.1)

item_avgs = compute_item_averages(R, beta_i=beta_i, add_laplace=add_noise_to_biases, laplace_scale=bias_noise_scale)
user_effects = compute_user_effects(R, item_avgs, beta_u=beta_u, add_laplace=add_noise_to_biases, laplace_scale=bias_noise_scale)

st.write("Example item averages (first 10):", np.round(item_avgs[:10], 3))
st.write("Example user effects (first 10):", np.round(user_effects[:10], 3))

# Manual SGD training options
st.markdown("### Step 4 — Manual SGD matrix factorization (explicit loops; educational)")
st.sidebar.header("SGD training options")
d = st.sidebar.slider("Latent dimension d", 2, 50, 5)
epochs = st.sidebar.slider("Epochs", 1, 50, 10)
lr = st.sidebar.number_input("Learning rate", value=0.01, format="%.4f")
reg = st.sidebar.number_input("L2 reg (lambda)", value=0.02, format="%.4f")
add_laplace_to_error = st.sidebar.checkbox("Add Laplace noise to per-update error (illustrative)", value=True)
error_noise_scale = st.sidebar.number_input("Laplace scale for update noise", min_value=0.0, max_value=2.0, value=0.05)

# Button to train (manual SGD)
if st.button("Train MF with manual SGD (educational)"):
    st.markdown("**Running manual SGD.** This may be slow for large datasets. Progress (RMSE) per epoch shown below.")
    P, Q, history = manual_sgd_factorization(R, d=d, epochs=epochs, lr=lr, reg=reg,
                                            add_laplace_to_error=add_laplace_to_error, laplace_scale=error_noise_scale)
    R_pred = P.dot(Q.T)
    final_rmse = rmse_on_observed(R, R_pred)
    st.success(f"Training finished. Final RMSE on observed entries: {final_rmse:.4f}")

    # Save to session for recommendation usage
    st.session_state['P'] = P
    st.session_state['Q'] = Q
    st.session_state['item_avgs'] = item_avgs
    st.session_state['user_effects'] = user_effects
    st.session_state['R'] = R
    st.session_state['df'] = df
    st.session_state['users'] = users
    st.session_state['items'] = items
    st.session_state['history'] = history

    # Plot RMSE history (simple)
    st.line_chart(np.array(history))

# If model present in session, show recommendation UI
if all(k in st.session_state for k in ['P', 'Q', 'item_avgs', 'user_effects', 'R']):
    st.markdown("### Step 5 — Get recommendations")
    P = st.session_state['P']; Q = st.session_state['Q']
    item_avgs = st.session_state['item_avgs']; user_effects = st.session_state['user_effects']
    R = st.session_state['R']; df_proc = st.session_state['df']
    users = st.session_state['users']; items = st.session_state['items']

    st.write("**What inputs you must provide to get recommendations:**")
    st.markdown("""
     1. **User ID** (the original UserID present in the dataset) — the app will remap it internally.
     2. **Movie ID** (the original MovieID present in the dataset) — this is the movie you want similar recommendations for.
     3. Click **Get Recommendations**.
     """)
    # Input fields
    user_input = st.number_input("Enter original UserID (must exist in dataset)", min_value=int(df_proc['UserID'].min()),
                                 max_value=int(df_proc['UserID'].max()), value=int(df_proc['UserID'].min()))
    movie_input = st.number_input("Enter original MovieID (must exist in dataset)", min_value=int(df_proc['MovieID'].min()),
                                 max_value=int(df_proc['MovieID'].max()), value=int(df_proc['MovieID'].min()))
    top_n = st.number_input("Number of similar movies to return", min_value=1, max_value=50, value=10)

    if st.button("Get Recommendations"):
        # remap to idx values
        try:
            u_idx = np.where(users == user_input)[0][0]
            i_idx = np.where(items == movie_input)[0][0]
        except IndexError:
            st.error("Entered UserID or MovieID not found in dataset (they must be present).")
            st.stop()

        # Show predicted rating for given user/movie
        given_pred = predict_rating_user_item(P, Q, item_avgs, user_effects, int(u_idx), int(i_idx))
        st.write(f"Predicted rating by the model for user {user_input} on movie {movie_input} = {given_pred:.3f}")

        # Recommend similar
        df_rec = recommend_similar_by_predicted_rating(P, Q, item_avgs, user_effects, R, int(u_idx), int(i_idx), top_n=top_n)
        # Map back i_idx to original MovieID for display
        df_rec['MovieID'] = df_rec['i_idx'].apply(lambda idx: items[idx])
        df_rec = df_rec[['MovieID', 'PredictedRating', 'AbsDiffFromGiven']]
        st.write(f"Top {top_n} movies whose predicted rating for user {user_input} is closest to movie {movie_input}:")
        st.dataframe(df_rec.reset_index(drop=True))

    st.markdown("### Notes / Teaching points")
    st.markdown("""
      - The dense R matrix was built after remapping IDs to contiguous indices; this is necessary for small educational examples.
      - Item averages & user effects were computed explicitly and you can toggle adding Laplace noise to see how they change.
      - Manual SGD here updates P and Q entry-by-entry — it's simple to read but not optimized for speed.
      - Adding Laplace noise to errors or biases is **illustrative**; proper DP requires careful sensitivity and composition analysis.
      - To scale to real datasets (MovieLens 100k+), switch to sparse structures and vectorized updates or use libraries.
    """)

else:
    st.info("Train the model (manual SGD) first to enable recommendations.")

###############################################################################
# --------------------------------- Footer -----------------------------------
###############################################################################
st.markdown("---")
st.markdown("**Educational app**: Manual MF + Laplace noise. Not production DP. Intended for learning the mechanics.")
